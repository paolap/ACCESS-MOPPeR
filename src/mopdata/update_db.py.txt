#!/usr/bin/env python
# Copyright 2024 ARC Centre of Excellence for Climate Extremes (CLEX)
# Author: Paola Petrelli <paola.petrelli@utas.edu.au> for CLEX
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# contact: paola.petrelli@utas.edu.au
#
# last updated 08/10/2024
#
# Simple script to update file records in mopper.db database

import sqlite3
import json
import sys
import csv
import os

def get_rows(conn, exp):
    cursor = conn.cursor()
    cursor.execute("""SELECT filename,variable_id,ctable,calculation,
               
        resample,vin,status,ROWID FROM filelist WHERE exp_id == ?""",
                    (exp,))
    rows = cursor.fetchall()
    return rows


def update_status(conn, varid, ctable, old, new):
    cur = conn.cursor()
    cur.execute("""UPDATE filelist SET status = ? where 
        status == ? and variable_id == ? and ctable == ?""",
        (new, old, varid, ctable))
    print(f"Updated {cur.rowcount} rows\n")
    conn.commit()
    return


def update_file(conn, varid, ctable, fdate, status, allrows=False):
    cur = conn.cursor()
    if allrows is True:
        cur.execute(f"UPDATE filelist SET status='{status}'") 
    else:
        cur.execute("""UPDATE filelist SET status = ? where
            variable_id == ? and ctable == ? and tstart == ?""",
            (status, varid, ctable, fdate))
    updated = cur.rowcount
    conn.commit()
    return updated


def update_map(conn, varid, ctable):
    """Read mappings for variable from map file and
    update them in filelist
    """
    keys = ['frequency','realm','timeshot','calculation',
            'positive', 'resample', 'axes']
    keys2 = {'vin': 'input_vars', 'in_units': 'units'}
    fname = f"maps/{ctable}.json"
    with open(fname, 'r') as f:
         data = json.load(f)
    found = False
    for row in data:
        if row['cmor_var'] == varid:
            found = True
            break
    if found: 
        args = {k: row[k] for k in keys}
        for k,v in keys2.items():
            args[k] = row[v]
        # you need to manually add datadir to the maps
        # if you want to be update infile!
        if 'datadir' in row.keys():
            paths = row['file_structure'].split()
            infile = ''
            for x in paths:
                infile += f"{row['datadir']}/{x} "
            args['infile'] = infile
        cur = conn.cursor()
        values = []
        sql = f"UPDATE filelist SET"
        for k,v in args.items(): 
            sql += f" {k} = ?," 
            values.append(v)
        values.extend([varid, ctable])
        sql = sql[:-1] + f" WHERE variable_id == ? and ctable == ?" 
        cur.execute(sql, tuple(values))
        print(f"Updated {cur.rowcount} rows\n")
        conn.commit()
    else:
        print(f"No match for {varid} in maps/{ctable}.json")
    return


def get_summary(rows):
    """Get a summary of variables with issues
       by status messages
    """
    status_msg = {'unprocessed': "file ready to be processed",
        'data_unavailable': "incomplete input data", 
        'processed': "file already processed",
        'unknown_return_code': "processing failed with unidentified error",
        'processing_failed': "processing failed with unidentified error",
        'calculation_failed': "processing failed with unidentified error",
        'file_mismatch': "produced but file name does not match expected",
        'cmor_error': "cmor variable definition or write failed",
        'mapping_error': "problem with variable mapping and/or definition"}

    flist = {k:set() for k in status_msg.keys()}
    for r in rows:
        if r[6] != 'processed':
            flist[r[6]].add((r[1],r[2],r[3],r[4],r[5]))
    for k,value in flist.items():
        if len(value) > 0:
           print(status_msg[k] + "\n")
           for v in value:
               print(f"Variable {v[0]} - {v[1]}; " +
                     f"calculation: {v[2]} with input {v[4]}; "+
                     f"resample: {v[3]}") 
    print("")
    return flist


def process_var(conn, flist, varname=None):
    """For each variable ask if user want to update status to
    processed or unprocessed
    """
    if varname is not None:
        final = {}
        for k in flist.keys():
            final[k] = (v for v in flist[k] if v[0] == varname)
    else:
        final = flist
    for k,value in final.items():
        for v in value:
            print(f"status of {v[0]} - {v[1]} is {k}")
            ans = input("Update status to unprocessed? (Y/N)  ")
            if ans == 'N':
                ans = input("Update status to processed? (Y/N)  ")
                if ans == 'Y':
                    update_status(conn, v[0], v[1], k, 'processed')
                else:
                    print(f"No updates for {v[0]}-{v[1]}")
            else:
                update_status(conn, v[0], v[1], k, 'unprocessed')
                ans = input("Update mapping? (Y/N)  ")
                if ans == 'Y':
                    print('variable',v)
                    update_map(conn, v[0], v[1]) 
                
    return


def bulk_update(conn, unprocessed=False):
    """Update status of all variables either using failed and success
    files or reset all vars to unprocessed.
    """
    if unprocessed is True:
        st = update_file(conn,'','','','unprocessed', allrows=True)
        print(f"{st} files updated\n")
    else:
        success = []
        failed = []
        suc_count = 0
        fail_count = 0
        if os.path.isfile('failed.csv') is True:
            with open('failed.csv', 'r') as f:
                reader = csv.reader(f, delimiter=',')
                for row in reader:
                    failed.append((row[1], row[0], row[2]))
            for f in failed:
                st = update_file(conn,f[0],f[1],f[2],'unprocessed')
                fail_count += st
            print(f"{len(failed)} failed files and {fail_count} updated\n")
        else:
            print("no failed.csv file\n")
        if os.path.isfile('success.csv') is True:
            with open('success.csv', 'r') as f:
                reader = csv.reader(f, delimiter=',')
                for row in reader:
                    success.append((row[1], row[0], row[2]))
            for f in success:
                st = update_file(conn,f[0],f[1],f[2],'processed')
                suc_count += st
            print(f"{len(success)} successful files and {suc_count} updated\n")
        else:
            print("no success.csv file\n")
    return

exp = sys.argv[1]
if len(sys.argv) == 3:
    dbname = sys.argv[2]
else:
    dbname = 'mopper.db'
conn=sqlite3.connect(dbname, timeout=200.0)
ans = input("Update db based on success/failed? (Y/N)  ")
if ans == 'Y':
    bulk_update(conn)
else:
    ans = input("Reset all variables to unprocessed? (Y/N)  ")
    if ans == 'Y':
        bulk_update(conn, unprocessed=True)
    else:
        rows = get_rows(conn, exp)
        flist =  get_summary(rows)
        ans = input("Update specific variable? (Y/N)  ")
        if ans == 'Y':
            varname = input("Variable: ")
            while varname != '':
                process_var(conn, flist, varname=varname)
                varname = input("Variable: (name or Enter to stop)  ")
        elif ans == 'N':
            process_var(conn, flist)
